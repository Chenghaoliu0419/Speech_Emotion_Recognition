{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMEAchAEYx/rAyrkJ671veq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"__OSu16n_Cma"},"outputs":[],"source":["from transformers import AutoModelForAudioClassification\n","import librosa, torch\n","\n","#load model\n","model = AutoModelForAudioClassification.from_pretrained(\"3loi/SER-Odyssey-Baseline-WavLM-Multi-Attributes\", trust_remote_code=True)\n","\n","#get mean/std\n","mean = model.config.mean\n","std = model.config.std\n","\n","\n","#load an audio file\n","audio_path = \"/path/to/audio.wav\"\n","raw_wav, _ = librosa.load(audio_path, sr=model.config.sampling_rate)\n","\n","#normalize the audio by mean/std\n","norm_wav = (raw_wav - mean) / (std+0.000001)\n","\n","#generate the mask\n","mask = torch.ones(1, len(norm_wav))\n","\n","#batch it (add dim)\n","wavs = torch.tensor(norm_wav).unsqueeze(0)\n","\n","\n","#predict\n","with torch.no_grad():\n","    pred = model(wavs, mask)\n","\n","print(model.config.id2label)\n","print(pred)\n","#{0: 'arousal', 1: 'dominance', 2: 'valence'}\n","#tensor([[0.3670, 0.4553, 0.4240]])\n"]}]}