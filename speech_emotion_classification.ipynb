{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620}],"dockerImageVersionId":30176,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **SPEECH EMOTION RECOGNITION**\n\n\n**TABLE OF CONTENTS**\n* INTRODUCTION\n* EXPLORATORY DATA ANALYSIS (EDA)\n* DATA AUGMENTATION\n* FEATURE EXTRACTION\n* MODEL","metadata":{}},{"cell_type":"markdown","source":"# INTRODUCTION\n\n**PROBLEM STATEMENT**\n\nThe purpose is to recognize the emotion and affective state of the speaker from his/her speech signal. \n\n**DATA SOURCE USED**\n\nWe have used the RAVDESS dataset in this project.It is one of the more common dataset used for this excercise by others. It's well liked because of its quality of speakers, recording and it has 24 actors of different genders.\nHere's the filename identifiers as per the official RAVDESS website:\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\nHere's an example of an audio filename. 02-01-06-01-02-01-12.mp4","metadata":{}},{"cell_type":"markdown","source":"# EXPLORATORY DATA ANALYSIS\n\nThe key features of the audio data are namely, MFCC (Mel Frequency Cepstral Coefficients) and Mel Spectrogram \n\n* MFCC (Mel Frequency Cepstral Coefficients)- \nMFCC is important feature extraction when using speech data,Mel scale is a scale that relates the perceived frequency of a tone to the real measured frequency. It scales the frequency so that you can fit greater carefully what the human ear can hear \n\n\n\n* Mel Spectrogram- \nA Fast Fourier Transform is computed on overlapping windowed segments of the signal, and what is the spectrogram , Spectrogram is visual way of representation of signal strength and also use for display the frequency sound waves.\n\n\n\nFor the EDA we have used MFCC and Mel Spectogram","metadata":{}},{"cell_type":"code","source":"#IMPORT THE LIBRARIES\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nimport IPython.display as ipd\nfrom IPython.display import Audio\nimport keras\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM,BatchNormalization , GRU\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import SGD\n\n\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:24:50.18789Z","iopub.execute_input":"2022-04-04T11:24:50.188197Z","iopub.status.idle":"2022-04-04T11:24:58.439414Z","shell.execute_reply.started":"2022-04-04T11:24:50.188166Z","shell.execute_reply":"2022-04-04T11:24:58.438017Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# path to the directory\nRAVD = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:24:58.44567Z","iopub.execute_input":"2022-04-04T11:24:58.446361Z","iopub.status.idle":"2022-04-04T11:24:58.454236Z","shell.execute_reply.started":"2022-04-04T11:24:58.446318Z","shell.execute_reply":"2022-04-04T11:24:58.45271Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dirl_list = os.listdir(RAVD)\ndirl_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dirl_list:\n    fname = os.listdir(RAVD + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAVD + i + '/' + f)\n\n        \nRAVD_df = pd.DataFrame(emotion)\nRAVD_df = RAVD_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAVD_df = pd.concat([pd.DataFrame(gender),RAVD_df],axis=1)\nRAVD_df.columns = ['gender','emotion']\nRAVD_df['labels'] =RAVD_df.gender + '_' + RAVD_df.emotion\nRAVD_df['source'] = 'RAVDESS'  \nRAVD_df = pd.concat([RAVD_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAVD_df = RAVD_df.drop(['gender', 'emotion'], axis=1)\nRAVD_df.labels.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:24:58.458351Z","iopub.execute_input":"2022-04-04T11:24:58.4589Z","iopub.status.idle":"2022-04-04T11:24:58.981673Z","shell.execute_reply.started":"2022-04-04T11:24:58.458856Z","shell.execute_reply":"2022-04-04T11:24:58.980691Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.title('Count of Emotions', size=16)\nsns.countplot(RAVD_df.labels)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nplt.xticks(rotation=45)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:24:58.984694Z","iopub.execute_input":"2022-04-04T11:24:58.984984Z","iopub.status.idle":"2022-04-04T11:24:59.320838Z","shell.execute_reply.started":"2022-04-04T11:24:58.984943Z","shell.execute_reply":"2022-04-04T11:24:59.319908Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Female Happy\nfRA1= RAVD + 'Actor_08/03-01-03-02-02-01-08.wav'\ndata, sr = librosa.load(fRA1)\nipd.Audio(fRA1) \n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:24:59.322606Z","iopub.execute_input":"2022-04-04T11:24:59.323136Z","iopub.status.idle":"2022-04-04T11:25:01.08629Z","shell.execute_reply.started":"2022-04-04T11:24:59.323092Z","shell.execute_reply":"2022-04-04T11:25:01.083686Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CREATE LOG MEL SPECTROGRAM\nplt.figure(figsize=(10, 5))\nspectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \nspectrogram = librosa.power_to_db(spectrogram)\nlibrosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\nplt.title('Mel Spectrogram - Female Happy')\nplt.colorbar(format='%+2.0f dB')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:01.087795Z","iopub.execute_input":"2022-04-04T11:25:01.088194Z","iopub.status.idle":"2022-04-04T11:25:01.669936Z","shell.execute_reply.started":"2022-04-04T11:25:01.088148Z","shell.execute_reply":"2022-04-04T11:25:01.669062Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Female Fear\nfRA2=RAVD +'Actor_08/03-01-06-01-01-01-08.wav'\ndata, sr = librosa.load(fRA2)\nipd.Audio(fRA2) ","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:01.674336Z","iopub.execute_input":"2022-04-04T11:25:01.675028Z","iopub.status.idle":"2022-04-04T11:25:02.13922Z","shell.execute_reply.started":"2022-04-04T11:25:01.674977Z","shell.execute_reply":"2022-04-04T11:25:02.13741Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CREATE LOG MEL SPECTROGRAM\nplt.figure(figsize=(10, 5))\nspectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \nspectrogram = librosa.power_to_db(spectrogram)\nlibrosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\nplt.title('Mel Spectrogram - Female Fear')\nplt.colorbar(format='%+2.0f dB');","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:02.141317Z","iopub.execute_input":"2022-04-04T11:25:02.141989Z","iopub.status.idle":"2022-04-04T11:25:02.601178Z","shell.execute_reply.started":"2022-04-04T11:25:02.141944Z","shell.execute_reply":"2022-04-04T11:25:02.600058Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we compare the waveplots of happy and fearful tracks","metadata":{}},{"cell_type":"code","source":"# Female Disgust\nfRA1 =RAVD +'Actor_20/03-01-08-02-02-02-20.wav'\ndata, sr = librosa.load(fRA1)\nipd.Audio(fRA1) ","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:02.602918Z","iopub.execute_input":"2022-04-04T11:25:02.604046Z","iopub.status.idle":"2022-04-04T11:25:02.833014Z","shell.execute_reply.started":"2022-04-04T11:25:02.604003Z","shell.execute_reply":"2022-04-04T11:25:02.831357Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CREATE LOG MEL SPECTROGRAM\nplt.figure(figsize=(10, 5))\nspectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \nspectrogram = librosa.power_to_db(spectrogram)\nlibrosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\nplt.title('Mel Spectrogram - Female Disgust')\nplt.colorbar(format='%+2.0f dB');","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:02.837459Z","iopub.execute_input":"2022-04-04T11:25:02.838069Z","iopub.status.idle":"2022-04-04T11:25:03.28233Z","shell.execute_reply.started":"2022-04-04T11:25:02.838029Z","shell.execute_reply":"2022-04-04T11:25:03.281345Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Male Fearfull\nfRA1 = RAVD + 'Actor_19/03-01-04-01-02-01-19.wav'\ndata, sr = librosa.load(fRA1)\nipd.Audio(fRA1) ","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:03.283669Z","iopub.execute_input":"2022-04-04T11:25:03.285692Z","iopub.status.idle":"2022-04-04T11:25:03.503263Z","shell.execute_reply.started":"2022-04-04T11:25:03.285646Z","shell.execute_reply":"2022-04-04T11:25:03.502501Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CREATE LOG MEL SPECTROGRAM\nplt.figure(figsize=(10, 5))\nspectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \nspectrogram = librosa.power_to_db(spectrogram)\nlibrosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\nplt.title('Mel Spectrogram - Male Fearfull')\nplt.colorbar(format='%+2.0f dB');","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:03.50485Z","iopub.execute_input":"2022-04-04T11:25:03.505806Z","iopub.status.idle":"2022-04-04T11:25:03.947541Z","shell.execute_reply.started":"2022-04-04T11:25:03.505752Z","shell.execute_reply":"2022-04-04T11:25:03.946455Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gender - Female; Emotion - Angry \npath = \"../input/ravdess-emotional-speech-audio/Actor_18/03-01-05-01-01-01-18.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n\n# MFCC\nplt.figure(figsize=(16, 10))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:03.949209Z","iopub.execute_input":"2022-04-04T11:25:03.949475Z","iopub.status.idle":"2022-04-04T11:25:04.356Z","shell.execute_reply.started":"2022-04-04T11:25:03.949425Z","shell.execute_reply":"2022-04-04T11:25:04.354983Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gender - Male; Emotion - Angry \npath = \"../input/ravdess-emotional-speech-audio/Actor_17/03-01-05-01-01-02-17.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n\n# MFCC\nplt.figure(figsize=(16, 10))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:04.357712Z","iopub.execute_input":"2022-04-04T11:25:04.358221Z","iopub.status.idle":"2022-04-04T11:25:04.766564Z","shell.execute_reply.started":"2022-04-04T11:25:04.358177Z","shell.execute_reply":"2022-04-04T11:25:04.765586Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gender - Female; Emotion - angry\npath = \"../input/ravdess-emotional-speech-audio/Actor_18/03-01-05-01-01-01-18.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Gender - Male; Emotion - angry\npath = \"../input/ravdess-emotional-speech-audio/Actor_17/03-01-05-01-01-02-17.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# Plot the two audio waves together\nplt.figure(figsize=(16,10))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:04.768235Z","iopub.execute_input":"2022-04-04T11:25:04.768861Z","iopub.status.idle":"2022-04-04T11:25:05.200973Z","shell.execute_reply.started":"2022-04-04T11:25:04.768803Z","shell.execute_reply":"2022-04-04T11:25:05.200057Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gender - Female; Emotion - Surprised\npath = \"../input/ravdess-emotional-speech-audio/Actor_20/03-01-08-02-01-02-20.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Gender - Male; Emotion - Surprised\npath = \"../input/ravdess-emotional-speech-audio/Actor_21/03-01-08-02-01-01-21.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# Plot the two audio waves together\nplt.figure(figsize=(16,10))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:05.202816Z","iopub.execute_input":"2022-04-04T11:25:05.203793Z","iopub.status.idle":"2022-04-04T11:25:05.687567Z","shell.execute_reply.started":"2022-04-04T11:25:05.203747Z","shell.execute_reply":"2022-04-04T11:25:05.686456Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Again we combine the emotion of suprised male female then again male has higher pitch**","metadata":{}},{"cell_type":"markdown","source":"# DATA AUGMENTATION\n\n* Data augmentation is the process by which we create new polymerized data samples by adding small disturbance on our initial training set.\n* To generate polymerized data for audio, we can apply noise injection, shifting time, changing pitch and speed.\n* The objective is to make our model invariant to those disturbance and enhace its ability to generalize.\nIn order to this to work adding the disturbance must conserve the same label as the original training sample.","metadata":{}},{"cell_type":"code","source":"# NOISE\ndef noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n# STRETCH\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate)\n# SHIFT\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n# PITCH\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:05.689212Z","iopub.execute_input":"2022-04-04T11:25:05.689793Z","iopub.status.idle":"2022-04-04T11:25:05.699414Z","shell.execute_reply.started":"2022-04-04T11:25:05.689746Z","shell.execute_reply":"2022-04-04T11:25:05.698112Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trying different functions above\npath = np.array(RAVD_df['path'])[471]\ndata, sample_rate = librosa.load(path)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:05.701527Z","iopub.execute_input":"2022-04-04T11:25:05.702006Z","iopub.status.idle":"2022-04-04T11:25:05.950611Z","shell.execute_reply.started":"2022-04-04T11:25:05.701932Z","shell.execute_reply":"2022-04-04T11:25:05.94954Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NORMAL AUDIO\n\n\nimport librosa.display\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveshow(y=data, sr=sample_rate)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:05.95217Z","iopub.execute_input":"2022-04-04T11:25:05.95262Z","iopub.status.idle":"2022-04-04T11:25:06.423549Z","shell.execute_reply.started":"2022-04-04T11:25:05.952574Z","shell.execute_reply":"2022-04-04T11:25:06.422596Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# AUDIO WITH NOISE\nx = noise(data)\nplt.figure(figsize=(12,5))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:06.425268Z","iopub.execute_input":"2022-04-04T11:25:06.425867Z","iopub.status.idle":"2022-04-04T11:25:06.927777Z","shell.execute_reply.started":"2022-04-04T11:25:06.42579Z","shell.execute_reply":"2022-04-04T11:25:06.926596Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STRETCHED AUDIO\nx = stretch(data)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:06.929767Z","iopub.execute_input":"2022-04-04T11:25:06.930438Z","iopub.status.idle":"2022-04-04T11:25:07.945491Z","shell.execute_reply.started":"2022-04-04T11:25:06.930393Z","shell.execute_reply":"2022-04-04T11:25:07.944449Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SHIFTED AUDIO\nx = shift(data)\nplt.figure(figsize=(12,5))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:07.947389Z","iopub.execute_input":"2022-04-04T11:25:07.947752Z","iopub.status.idle":"2022-04-04T11:25:08.655241Z","shell.execute_reply.started":"2022-04-04T11:25:07.947707Z","shell.execute_reply":"2022-04-04T11:25:08.654023Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# AUDIO WITH PITCH\nx = pitch(data, sample_rate)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:08.657145Z","iopub.execute_input":"2022-04-04T11:25:08.657519Z","iopub.status.idle":"2022-04-04T11:25:09.280415Z","shell.execute_reply.started":"2022-04-04T11:25:08.657473Z","shell.execute_reply":"2022-04-04T11:25:09.279465Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**After listening to all audio we determine to use noise, shift and stretch**","metadata":{}},{"cell_type":"markdown","source":"# FEATURE EXTRACTION","metadata":{}},{"cell_type":"code","source":"\ndef feat_ext(data):\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n    return mfcc\n\ndef get_feat(path):\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    # normal data\n    res1 = feat_ext(data)\n    result = np.array(res1)\n    #data with noise\n    noise_data = noise(data)\n    res2 = feat_ext(noise_data)\n    result = np.vstack((result, res2))\n    #data with stretch and pitch\n    new_data = stretch(data)\n    data_stretch_pitch = pitch(new_data, sample_rate)\n    res3 = feat_ext(data_stretch_pitch)\n    result = np.vstack((result, res3))\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:09.282439Z","iopub.execute_input":"2022-04-04T11:25:09.283098Z","iopub.status.idle":"2022-04-04T11:25:09.293019Z","shell.execute_reply.started":"2022-04-04T11:25:09.283053Z","shell.execute_reply":"2022-04-04T11:25:09.291796Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RAVD_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:09.294424Z","iopub.execute_input":"2022-04-04T11:25:09.295026Z","iopub.status.idle":"2022-04-04T11:25:09.316425Z","shell.execute_reply.started":"2022-04-04T11:25:09.294981Z","shell.execute_reply":"2022-04-04T11:25:09.315491Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, Y = [], []\nfor path, emotion in zip(RAVD_df['path'], RAVD_df['labels']):\n    feature = get_feat(path)\n    for ele in feature:\n        X.append(ele)\n        Y.append(emotion)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:25:09.317792Z","iopub.execute_input":"2022-04-04T11:25:09.318088Z","iopub.status.idle":"2022-04-04T11:37:12.208584Z","shell.execute_reply.started":"2022-04-04T11:25:09.318045Z","shell.execute_reply":"2022-04-04T11:37:12.207476Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Emotions = pd.DataFrame(X)\nEmotions['labels'] = Y\nEmotions.to_csv('emotion.csv', index=False)\nEmotions.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:12.210109Z","iopub.execute_input":"2022-04-04T11:37:12.21065Z","iopub.status.idle":"2022-04-04T11:37:12.484338Z","shell.execute_reply.started":"2022-04-04T11:37:12.210591Z","shell.execute_reply":"2022-04-04T11:37:12.48348Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# can use this directly from saved feature .csv file\nEmotions = pd.read_csv('./emotion.csv')\nEmotions.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:12.490911Z","iopub.execute_input":"2022-04-04T11:37:12.491163Z","iopub.status.idle":"2022-04-04T11:37:12.54231Z","shell.execute_reply.started":"2022-04-04T11:37:12.491137Z","shell.execute_reply":"2022-04-04T11:37:12.541227Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA PREPROCESSING","metadata":{}},{"cell_type":"code","source":"X = Emotions.iloc[: ,:-1].values\nY = Emotions['labels'].values","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:12.544009Z","iopub.execute_input":"2022-04-04T11:37:12.544333Z","iopub.status.idle":"2022-04-04T11:37:12.553007Z","shell.execute_reply.started":"2022-04-04T11:37:12.544281Z","shell.execute_reply":"2022-04-04T11:37:12.550107Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# As this is a multiclass classification problem onehotencoding our Y\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:12.554711Z","iopub.execute_input":"2022-04-04T11:37:12.555411Z","iopub.status.idle":"2022-04-04T11:37:12.567596Z","shell.execute_reply.started":"2022-04-04T11:37:12.555366Z","shell.execute_reply":"2022-04-04T11:37:12.566568Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and Test Split \nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:12.569379Z","iopub.execute_input":"2022-04-04T11:37:12.569831Z","iopub.status.idle":"2022-04-04T11:37:12.582182Z","shell.execute_reply.started":"2022-04-04T11:37:12.569784Z","shell.execute_reply":"2022-04-04T11:37:12.580874Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reshape for LSTM \nX_train = x_train.reshape(x_train.shape[0] , x_train.shape[1] , 1)\nX_test = x_test.reshape(x_test.shape[0] , x_test.shape[1] , 1)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:12.584406Z","iopub.execute_input":"2022-04-04T11:37:12.584977Z","iopub.status.idle":"2022-04-04T11:37:12.590965Z","shell.execute_reply.started":"2022-04-04T11:37:12.584934Z","shell.execute_reply":"2022-04-04T11:37:12.589688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# scaling our data with sklearn's Standard scaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:12.593036Z","iopub.execute_input":"2022-04-04T11:37:12.593795Z","iopub.status.idle":"2022-04-04T11:37:12.608763Z","shell.execute_reply.started":"2022-04-04T11:37:12.593753Z","shell.execute_reply":"2022-04-04T11:37:12.607583Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# IMPLOYING MODELS","metadata":{}},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclf3 = DecisionTreeClassifier()\n\nclf3 = clf3.fit(x_train,y_train)\n\ny_pred = clf3.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:12.610462Z","iopub.execute_input":"2022-04-04T11:37:12.611054Z","iopub.status.idle":"2022-04-04T11:37:12.778494Z","shell.execute_reply.started":"2022-04-04T11:37:12.611007Z","shell.execute_reply":"2022-04-04T11:37:12.777649Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training set score: {:.3f}\".format(clf3.score(x_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(clf3.score(x_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:12.780047Z","iopub.execute_input":"2022-04-04T11:37:12.780351Z","iopub.status.idle":"2022-04-04T11:37:12.80664Z","shell.execute_reply.started":"2022-04-04T11:37:12.780309Z","shell.execute_reply":"2022-04-04T11:37:12.805563Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**After Using the decision tree model in our dataset then the model goes in overfitting he gave accuracy around .41**","metadata":{}},{"cell_type":"markdown","source":"## **KNN**","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclf1=KNeighborsClassifier(n_neighbors=4)\nclf1.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:12.808282Z","iopub.execute_input":"2022-04-04T11:37:12.809097Z","iopub.status.idle":"2022-04-04T11:37:12.825704Z","shell.execute_reply.started":"2022-04-04T11:37:12.809052Z","shell.execute_reply":"2022-04-04T11:37:12.824836Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred=clf1.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:12.82729Z","iopub.execute_input":"2022-04-04T11:37:12.828041Z","iopub.status.idle":"2022-04-04T11:37:13.563277Z","shell.execute_reply.started":"2022-04-04T11:37:12.828Z","shell.execute_reply":"2022-04-04T11:37:13.562406Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training set score: {:.3f}\".format(clf1.score(x_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(clf1.score(x_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:13.565174Z","iopub.execute_input":"2022-04-04T11:37:13.565472Z","iopub.status.idle":"2022-04-04T11:37:16.425573Z","shell.execute_reply.started":"2022-04-04T11:37:13.565431Z","shell.execute_reply":"2022-04-04T11:37:16.424415Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Using KNN in our dataset the overall training is quite good but not for deployment and test accuracy has .50**","metadata":{}},{"cell_type":"markdown","source":"## **MLP CLASSIFIER**","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nclf2=MLPClassifier(alpha=0.01, batch_size=270, epsilon=1e-08, hidden_layer_sizes=(400,), learning_rate='adaptive', max_iter=400)\nclf2.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:16.426988Z","iopub.execute_input":"2022-04-04T11:37:16.427877Z","iopub.status.idle":"2022-04-04T11:37:45.10567Z","shell.execute_reply.started":"2022-04-04T11:37:16.427799Z","shell.execute_reply":"2022-04-04T11:37:45.104671Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training set score: {:.3f}\".format(clf2.score(x_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(clf2.score(x_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:45.107355Z","iopub.execute_input":"2022-04-04T11:37:45.10792Z","iopub.status.idle":"2022-04-04T11:37:45.167355Z","shell.execute_reply.started":"2022-04-04T11:37:45.107876Z","shell.execute_reply":"2022-04-04T11:37:45.166283Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Using MLP Classifier he gave good accuracy in training dataset but not good for our test set, So we did not use for deployment.** ","metadata":{}},{"cell_type":"markdown","source":"# GRU","metadata":{}},{"cell_type":"code","source":"model03 = Sequential()\nmodel03.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nmodel03.add(Dropout(0.3))\nmodel03.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nmodel03.add(Dropout(0.3))\nmodel03.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nmodel03.add(Dropout(0.3))\nmodel03.add(GRU(units=50, activation='tanh'))\nmodel03.add(Dropout(0.3))\nmodel03.add(Dense(units=14))\nmodel03.compile(optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=False),loss='mean_squared_error',metrics=['accuracy'])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:45.172859Z","iopub.execute_input":"2022-04-04T11:37:45.176149Z","iopub.status.idle":"2022-04-04T11:37:49.088956Z","shell.execute_reply.started":"2022-04-04T11:37:45.176094Z","shell.execute_reply":"2022-04-04T11:37:49.088003Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model03.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:49.090794Z","iopub.execute_input":"2022-04-04T11:37:49.091102Z","iopub.status.idle":"2022-04-04T11:37:49.106455Z","shell.execute_reply.started":"2022-04-04T11:37:49.091061Z","shell.execute_reply":"2022-04-04T11:37:49.105449Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model03.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=200)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:37:49.107911Z","iopub.execute_input":"2022-04-04T11:37:49.108229Z","iopub.status.idle":"2022-04-04T11:38:35.812212Z","shell.execute_reply.started":"2022-04-04T11:37:49.108189Z","shell.execute_reply":"2022-04-04T11:38:35.810963Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model03.evaluate(X_test,y_test)[1]*100 , \"%\")","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:38:35.813807Z","iopub.execute_input":"2022-04-04T11:38:35.814203Z","iopub.status.idle":"2022-04-04T11:38:36.189161Z","shell.execute_reply.started":"2022-04-04T11:38:35.814159Z","shell.execute_reply":"2022-04-04T11:38:36.188221Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**GRU(Gated Recurrent Unit) is important when we use speech dataset but in this model, we don't have good accuracy in training as well as testing so we do not take for further deployment.**","metadata":{}},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"model01=Sequential()\nmodel01.add(LSTM(70,return_sequences=True,input_shape=(20,1)))\nmodel01.add(LSTM(50,return_sequences=True))\nmodel01.add(LSTM(60))\nmodel01.add(Dense(14))\nmodel01.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:38:36.190777Z","iopub.execute_input":"2022-04-04T11:38:36.191408Z","iopub.status.idle":"2022-04-04T11:38:36.996016Z","shell.execute_reply.started":"2022-04-04T11:38:36.191356Z","shell.execute_reply":"2022-04-04T11:38:36.995045Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model01.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:38:36.997463Z","iopub.execute_input":"2022-04-04T11:38:36.997786Z","iopub.status.idle":"2022-04-04T11:38:37.01195Z","shell.execute_reply.started":"2022-04-04T11:38:36.997748Z","shell.execute_reply":"2022-04-04T11:38:37.010452Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" model01.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=65,verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:38:37.013572Z","iopub.execute_input":"2022-04-04T11:38:37.013969Z","iopub.status.idle":"2022-04-04T11:39:38.6445Z","shell.execute_reply.started":"2022-04-04T11:38:37.013927Z","shell.execute_reply":"2022-04-04T11:39:38.643442Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model01.evaluate(X_test,y_test)[1]*100 , \"%\")","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:39:38.646259Z","iopub.execute_input":"2022-04-04T11:39:38.646581Z","iopub.status.idle":"2022-04-04T11:39:38.861245Z","shell.execute_reply.started":"2022-04-04T11:39:38.646522Z","shell.execute_reply":"2022-04-04T11:39:38.860208Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**So, After all, the model ran we use LSTM is important when using sequence data like speech, So using this it ran good on train set but in the test has not good.**","metadata":{}},{"cell_type":"markdown","source":"## **CNN**","metadata":{}},{"cell_type":"code","source":"#CNN\nx_traincnn =np.expand_dims(x_train, axis=2)\nx_testcnn= np.expand_dims(x_test, axis=2)\nx_traincnn.shape, y_train.shape, x_testcnn.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:39:38.862767Z","iopub.execute_input":"2022-04-04T11:39:38.863168Z","iopub.status.idle":"2022-04-04T11:39:38.871922Z","shell.execute_reply.started":"2022-04-04T11:39:38.86312Z","shell.execute_reply":"2022-04-04T11:39:38.870603Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#CNN\nimport tensorflow as tf \nmodel = Sequential()\nmodel.add(Conv1D(2048, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(BatchNormalization())\n\nmodel.add(LSTM(256, return_sequences=True))\n\nmodel.add(LSTM(128))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(14, activation='softmax'))\n\noptimiser = tf.keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=optimiser,\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:39:38.873621Z","iopub.execute_input":"2022-04-04T11:39:38.874684Z","iopub.status.idle":"2022-04-04T11:39:39.622889Z","shell.execute_reply.started":"2022-04-04T11:39:38.874641Z","shell.execute_reply":"2022-04-04T11:39:39.62196Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#CNN\nhistory = model.fit(x_traincnn, y_train, batch_size=64, epochs=150, validation_data=(x_testcnn, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:39:39.626292Z","iopub.execute_input":"2022-04-04T11:39:39.62656Z","iopub.status.idle":"2022-04-04T11:44:05.604435Z","shell.execute_reply.started":"2022-04-04T11:39:39.626502Z","shell.execute_reply":"2022-04-04T11:44:05.603481Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#CNN\nprint(\"Accuracy of our model on test data : \" , model.evaluate(x_testcnn,y_test)[1]*100 , \"%\")","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:44:05.606397Z","iopub.execute_input":"2022-04-04T11:44:05.611257Z","iopub.status.idle":"2022-04-04T11:44:06.066239Z","shell.execute_reply.started":"2022-04-04T11:44:05.61121Z","shell.execute_reply":"2022-04-04T11:44:06.065202Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**So Using all models we use last but not least CNN after using the CNN model we got good accuracy in the train as well as test if you saw the test accuracy above cell is quite good so we use further deployment** ","metadata":{}},{"cell_type":"markdown","source":"# Loss Accuracy Plot Using CNN","metadata":{}},{"cell_type":"code","source":"#CNN\nepochs = [i for i in range(150)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:44:06.068278Z","iopub.execute_input":"2022-04-04T11:44:06.068666Z","iopub.status.idle":"2022-04-04T11:44:06.499449Z","shell.execute_reply.started":"2022-04-04T11:44:06.068621Z","shell.execute_reply":"2022-04-04T11:44:06.498458Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#CNN\npred_test = model.predict(x_testcnn)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)\n\ndf = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:44:06.501255Z","iopub.execute_input":"2022-04-04T11:44:06.501873Z","iopub.status.idle":"2022-04-04T11:44:07.641365Z","shell.execute_reply.started":"2022-04-04T11:44:06.501828Z","shell.execute_reply":"2022-04-04T11:44:07.640506Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Confusion Matrix Using CNN","metadata":{}},{"cell_type":"code","source":"#CNN\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:44:07.643691Z","iopub.execute_input":"2022-04-04T11:44:07.644294Z","iopub.status.idle":"2022-04-04T11:44:08.84932Z","shell.execute_reply.started":"2022-04-04T11:44:07.644254Z","shell.execute_reply":"2022-04-04T11:44:08.848463Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification Report Using CNN","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:44:08.850932Z","iopub.execute_input":"2022-04-04T11:44:08.851447Z","iopub.status.idle":"2022-04-04T11:44:08.900782Z","shell.execute_reply.started":"2022-04-04T11:44:08.851393Z","shell.execute_reply":"2022-04-04T11:44:08.89984Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**After completing the all training part we got good accuracy using CNN so we plot the confusion matrix and classification report.**","metadata":{}},{"cell_type":"markdown","source":"## **SAVING THE MODEL**","metadata":{}},{"cell_type":"code","source":"model_name = 'model3.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n# Save model and weights\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:44:08.902186Z","iopub.execute_input":"2022-04-04T11:44:08.902689Z","iopub.status.idle":"2022-04-04T11:44:09.337229Z","shell.execute_reply.started":"2022-04-04T11:44:08.902643Z","shell.execute_reply":"2022-04-04T11:44:09.336224Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:44:09.338759Z","iopub.execute_input":"2022-04-04T11:44:09.339097Z","iopub.status.idle":"2022-04-04T11:44:09.353674Z","shell.execute_reply.started":"2022-04-04T11:44:09.339054Z","shell.execute_reply":"2022-04-04T11:44:09.352268Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.models import model_from_json\njson_file = open('model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"/kaggle/working/saved_models/model3.h5\")\nprint(\"Loaded model from disk\")","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:44:09.355941Z","iopub.execute_input":"2022-04-04T11:44:09.357685Z","iopub.status.idle":"2022-04-04T11:44:10.153343Z","shell.execute_reply.started":"2022-04-04T11:44:09.357642Z","shell.execute_reply":"2022-04-04T11:44:10.152238Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LIVE DEMO","metadata":{}},{"cell_type":"code","source":"data, sampling_rate = librosa.load(\"../input/ravdess-emotional-speech-audio/Actor_01/03-01-02-02-02-01-01.wav\")","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:44:10.154686Z","iopub.execute_input":"2022-04-04T11:44:10.155362Z","iopub.status.idle":"2022-04-04T11:44:10.463608Z","shell.execute_reply.started":"2022-04-04T11:44:10.155315Z","shell.execute_reply":"2022-04-04T11:44:10.462519Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport librosa\nimport glob ","metadata":{"execution":{"iopub.status.busy":"2022-04-04T12:04:10.256849Z","iopub.execute_input":"2022-04-04T12:04:10.257192Z","iopub.status.idle":"2022-04-04T12:04:10.263222Z","shell.execute_reply.started":"2022-04-04T12:04:10.257163Z","shell.execute_reply":"2022-04-04T12:04:10.261762Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nlibrosa.display.waveshow(data, sr=sampling_rate)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:45:34.923704Z","iopub.execute_input":"2022-04-04T11:45:34.924033Z","iopub.status.idle":"2022-04-04T11:45:35.39371Z","shell.execute_reply.started":"2022-04-04T11:45:34.924003Z","shell.execute_reply":"2022-04-04T11:45:35.392686Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#livedf= pd.DataFrame(columns=['feature'])\nX, sample_rate = librosa.load('../input/ravdess-emotional-speech-audio/Actor_08/03-01-01-01-01-01-08.wav', res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\nsample_rate = np.array(sample_rate)\nmfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\nfeaturelive = mfccs\nlivedf2 = featurelive","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:45:35.396136Z","iopub.execute_input":"2022-04-04T11:45:35.396518Z","iopub.status.idle":"2022-04-04T11:45:35.485263Z","shell.execute_reply.started":"2022-04-04T11:45:35.396433Z","shell.execute_reply":"2022-04-04T11:45:35.48426Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"livedf2= pd.DataFrame(data=livedf2)\nlivedf2 = livedf2.stack().to_frame().T\nlivedf2","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:45:35.490303Z","iopub.execute_input":"2022-04-04T11:45:35.490838Z","iopub.status.idle":"2022-04-04T11:45:35.549564Z","shell.execute_reply.started":"2022-04-04T11:45:35.49079Z","shell.execute_reply":"2022-04-04T11:45:35.548575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"twodim= np.expand_dims(livedf2, axis=2)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:45:35.555979Z","iopub.execute_input":"2022-04-04T11:45:35.556372Z","iopub.status.idle":"2022-04-04T11:45:35.566678Z","shell.execute_reply.started":"2022-04-04T11:45:35.556311Z","shell.execute_reply":"2022-04-04T11:45:35.565574Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"livepreds = loaded_model.predict(twodim, \n                         batch_size=32, \n                         verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:45:35.568681Z","iopub.execute_input":"2022-04-04T11:45:35.569691Z","iopub.status.idle":"2022-04-04T11:45:36.514921Z","shell.execute_reply.started":"2022-04-04T11:45:35.569647Z","shell.execute_reply":"2022-04-04T11:45:36.514075Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"livepreds","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:45:36.516336Z","iopub.execute_input":"2022-04-04T11:45:36.516962Z","iopub.status.idle":"2022-04-04T11:45:36.524869Z","shell.execute_reply.started":"2022-04-04T11:45:36.51691Z","shell.execute_reply":"2022-04-04T11:45:36.523678Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"livepreds.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:45:36.526898Z","iopub.execute_input":"2022-04-04T11:45:36.527583Z","iopub.status.idle":"2022-04-04T11:45:36.53921Z","shell.execute_reply.started":"2022-04-04T11:45:36.527481Z","shell.execute_reply":"2022-04-04T11:45:36.537857Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"livepredictions = (encoder.inverse_transform((livepreds)))\nlivepredictions","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:45:36.5413Z","iopub.execute_input":"2022-04-04T11:45:36.541793Z","iopub.status.idle":"2022-04-04T11:45:36.552795Z","shell.execute_reply.started":"2022-04-04T11:45:36.541748Z","shell.execute_reply":"2022-04-04T11:45:36.551595Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Challenges\n\n  **Large Speech Dataset to Handle**\n  \n  \n  **It is not an easy track to convert Speech to Emotion**\n  \n  \n  **Feelings are subjective, people may interpret them differently. It is difficult to define the concept of emotions**\n  ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}